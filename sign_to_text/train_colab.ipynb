{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bd63db9b",
   "metadata": {},
   "source": [
    "# ü§ü Sign-to-Text Training Notebook (GPU Version)\n",
    "\n",
    "This notebook trains the **Hybrid CTC-Attention Model V2** with dual decoders (GRU + Transformer) for sign language to text translation.\n",
    "\n",
    "## üîß Setup Instructions\n",
    "\n",
    "1. **Clone the repository** - Uses the `fixed` branch with all NaN-safe fixes\n",
    "2. **Configure paths** - Set `DATA_DIR` to your extracted landmarks folder\n",
    "3. **Adjust batch size** - Based on your GPU VRAM (see config cell)\n",
    "4. **Run training** - Checkpoints are saved automatically\n",
    "\n",
    "## üìä Model Features\n",
    "\n",
    "- **Dual Decoder**: GRU (fast) + Transformer (accurate)\n",
    "- **CTC + Attention Loss**: Hybrid training objective\n",
    "- **NaN-Safe**: All edge cases handled (short sequences, masked attention)\n",
    "- **FP16 Mixed Precision**: Faster training, less VRAM\n",
    "- **Gradient Checkpointing**: ~2x batch size with same memory\n",
    "\n",
    "## üíæ Required Data\n",
    "\n",
    "- **extracted_landmarks_v2/**: Preprocessed landmark files (.npy)\n",
    "  - 204 features per frame: hands (126) + body (12) + mouth (40) + head_pose (12) + eyes (14)\n",
    "  - With velocity + acceleration: 204 √ó 3 = 612 dims\n",
    "- **iSign_v1.1.csv**: Label mapping file\n",
    "\n",
    "## üñ•Ô∏è GPU Memory Guide\n",
    "\n",
    "| GPU VRAM | Batch Size | Grad Accum | Effective Batch |\n",
    "|----------|------------|------------|-----------------|\n",
    "| 8GB      | 12         | 2          | 24              |\n",
    "| 12GB     | 16         | 2          | 32              |\n",
    "| 16GB     | 24         | 2          | 48              |\n",
    "| 24GB     | 32         | 2          | 64              |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "086c5cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Sign-to-Text Training Notebook (College GPU Version)\n",
    "# ============================================================\n",
    "# This notebook trains the Hybrid CTC-Attention model with dual decoders\n",
    "# Optimized for: Any CUDA GPU with 8GB+ VRAM\n",
    "# ============================================================\n",
    "\n",
    "# Check GPU\n",
    "import subprocess\n",
    "result = subprocess.run(['nvidia-smi'], capture_output=True, text=True)\n",
    "print(result.stdout)\n",
    "\n",
    "import torch\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfdce995",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone repository from the 'fixed' branch\n",
    "# This contains all the NaN-safe fixes and v2 model updates\n",
    "import os\n",
    "\n",
    "REPO_DIR = \"/home/user/kortex_5th_sem\"  # Change this to your preferred location\n",
    "\n",
    "if not os.path.exists(REPO_DIR):\n",
    "    # Clone from 'fixed' branch specifically\n",
    "    !git clone --branch fixed https://github.com/Surya-Narayan-M/kortex_5th_sem.git {REPO_DIR}\n",
    "    print(f\"‚úÖ Cloned 'fixed' branch to {REPO_DIR}\")\n",
    "else:\n",
    "    # Pull latest changes\n",
    "    os.chdir(REPO_DIR)\n",
    "    !git checkout fixed\n",
    "    !git pull origin fixed\n",
    "    print(f\"‚úÖ Updated 'fixed' branch in {REPO_DIR}\")\n",
    "\n",
    "os.chdir(f\"{REPO_DIR}/sign_to_text\")\n",
    "print(f\"Working directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cc683a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies (if not already installed)\n",
    "!pip install tqdm pandas numpy scipy mediapipe --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "108db1b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CONFIGURE PATHS - EDIT THESE FOR YOUR ENVIRONMENT\n",
    "# ============================================================\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# ========== IMPORTANT: SET YOUR PATHS ==========\n",
    "# Option 1: If you have v2 extracted landmarks already\n",
    "DATA_DIR = \"/path/to/extracted_landmarks_v2\"  # <-- CHANGE THIS\n",
    "\n",
    "\n",
    "CSV_PATH = f\"{REPO_DIR}/data/iSign_v1.1.csv\"\n",
    "CHECKPOINT_DIR = \"./checkpoints_v2\"\n",
    "\n",
    "# Create checkpoint directory\n",
    "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
    "\n",
    "# Add project to path\n",
    "sys.path.insert(0, f\"{REPO_DIR}/sign_to_text\")\n",
    "sys.path.insert(0, f\"{REPO_DIR}/data\")\n",
    "\n",
    "# Verify paths\n",
    "print(\"=\" * 60)\n",
    "print(\"PATH CONFIGURATION\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Repository: {REPO_DIR}\")\n",
    "print(f\"Data dir: {DATA_DIR}\")\n",
    "print(f\"CSV path: {CSV_PATH}\")\n",
    "print(f\"Checkpoint dir: {CHECKPOINT_DIR}\")\n",
    "print()\n",
    "\n",
    "if os.path.exists(DATA_DIR):\n",
    "    num_files = len([f for f in os.listdir(DATA_DIR) if f.endswith('.npy')])\n",
    "    print(f\"‚úÖ Found {num_files} landmark files\")\n",
    "else:\n",
    "    print(f\"‚ö†Ô∏è Data directory not found: {DATA_DIR}\")\n",
    "    print(\"   You need to either:\")\n",
    "    print(\"   1. Upload extracted_landmarks_v2 folder, OR\")\n",
    "    print(\"   2. Run the landmark extraction cell below\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81728fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# [OPTIONAL] EXTRACT LANDMARKS FROM VIDEOS\n",
    "# ============================================================\n",
    "# Only run this if you don't have extracted_landmarks_v2 already!\n",
    "# This requires the raw video files\n",
    "\n",
    "EXTRACT_LANDMARKS = False  # Set to True to run extraction\n",
    "\n",
    "if EXTRACT_LANDMARKS:\n",
    "    import subprocess\n",
    "    \n",
    "    OUTPUT_DIR = \"./extracted_landmarks_v2\"\n",
    "    NUM_WORKERS = 8  # Adjust based on CPU cores\n",
    "    \n",
    "    print(\"Starting landmark extraction...\")\n",
    "    print(\"This will take several hours depending on video count and GPU speed\")\n",
    "    \n",
    "    cmd = [\n",
    "        \"python\", f\"{REPO_DIR}/data/extract_landmarks_v2.py\",\n",
    "        \"--input-dir\", VIDEOS_DIR,\n",
    "        \"--output-dir\", OUTPUT_DIR,\n",
    "        \"--workers\", str(NUM_WORKERS)\n",
    "    ]\n",
    "    \n",
    "    result = subprocess.run(cmd, capture_output=False)\n",
    "    \n",
    "    if result.returncode == 0:\n",
    "        print(f\"‚úÖ Extraction complete! Files saved to {OUTPUT_DIR}\")\n",
    "        DATA_DIR = OUTPUT_DIR\n",
    "    else:\n",
    "        print(\"‚ùå Extraction failed!\")\n",
    "else:\n",
    "    print(\"Skipping landmark extraction (EXTRACT_LANDMARKS=False)\")\n",
    "    print(\"Using existing landmarks from:\", DATA_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b39aad23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# TRAINING CONFIGURATION\n",
    "# ============================================================\n",
    "# This class mirrors TrainConfig from train_hybrid.py but with\n",
    "# paths adjusted for your college GPU environment\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "class GPUConfig:\n",
    "    \"\"\"Training configuration for college GPU\"\"\"\n",
    "    \n",
    "    # Data paths - ADJUST THESE\n",
    "    data_dir = Path(DATA_DIR)\n",
    "    csv_path = Path(CSV_PATH)\n",
    "    vocab_path = Path(f\"{REPO_DIR}/sign_to_text/vocabulary.json\")\n",
    "    checkpoint_dir = Path(CHECKPOINT_DIR)\n",
    "    \n",
    "    # Feature version: v2 with face landmarks\n",
    "    feature_version = 'v2'\n",
    "    \n",
    "    # Model architecture (must match saved checkpoints)\n",
    "    input_dim = 612  # 204 * 3 = 612 (hands + body + face + velocity + acceleration)\n",
    "    hidden_dim = 384\n",
    "    embedding_dim = 256\n",
    "    encoder_layers = 3\n",
    "    decoder_layers = 2\n",
    "    num_heads = 4\n",
    "    dropout = 0.4\n",
    "    \n",
    "    # Dual decoder settings\n",
    "    use_dual_decoder = True\n",
    "    primary_decoder = 'gru'\n",
    "    \n",
    "    # Gradient checkpointing (enables larger batches with less VRAM)\n",
    "    use_gradient_checkpointing = True\n",
    "    \n",
    "    # ========== ADJUST BASED ON YOUR GPU VRAM ==========\n",
    "    # 8GB GPU:  batch_size=12, gradient_accumulation=2 ‚Üí effective=24\n",
    "    # 12GB GPU: batch_size=16, gradient_accumulation=2 ‚Üí effective=32\n",
    "    # 16GB GPU: batch_size=24, gradient_accumulation=2 ‚Üí effective=48\n",
    "    # 24GB GPU: batch_size=32, gradient_accumulation=2 ‚Üí effective=64\n",
    "    \n",
    "    batch_size = 16  # Adjust based on GPU memory\n",
    "    gradient_accumulation = 2\n",
    "    \n",
    "    # Training hyperparameters\n",
    "    epochs = 80\n",
    "    learning_rate = 5e-4\n",
    "    min_lr = 1e-6\n",
    "    weight_decay = 1e-4\n",
    "    warmup_epochs = 5\n",
    "    \n",
    "    # CTC/Attention balance (decays over training)\n",
    "    ctc_weight_start = 0.3\n",
    "    ctc_weight_end = 0.1\n",
    "    ctc_weight_decay_epochs = 30\n",
    "    \n",
    "    # Dual decoder loss weights\n",
    "    gru_loss_weight = 0.6\n",
    "    transformer_loss_weight = 0.4\n",
    "    \n",
    "    # Label smoothing\n",
    "    label_smoothing = 0.1\n",
    "    \n",
    "    # Teacher forcing schedule\n",
    "    tf_start = 0.9\n",
    "    tf_end = 0.2\n",
    "    tf_decay_epochs = 15\n",
    "    \n",
    "    # Early stopping\n",
    "    patience = 15\n",
    "    min_delta = 0.001\n",
    "    \n",
    "    # Device\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    use_amp = True  # Mixed precision (FP16)\n",
    "    \n",
    "    # Data loading\n",
    "    num_workers = 4\n",
    "    pin_memory = True\n",
    "    prefetch_factor = 2\n",
    "    persistent_workers = True\n",
    "    \n",
    "    # Logging\n",
    "    log_interval = 50\n",
    "    val_interval = 1\n",
    "    save_interval = 1\n",
    "    log_dir = Path(\"./logs\")\n",
    "    \n",
    "    # Sequence limits\n",
    "    max_src_len = 500\n",
    "    max_tgt_len = 100\n",
    "    \n",
    "    # Data augmentation\n",
    "    use_augmentation = True\n",
    "    augmentation_intensity = 'medium'\n",
    "    \n",
    "    # Dataset subset (for faster baseline training)\n",
    "    use_subset = True\n",
    "    subset_ratio = 0.40  # Use 40% of data\n",
    "    \n",
    "    # Validation split\n",
    "    val_ratio = 0.15\n",
    "    seed = 42\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"GPU TRAINING CONFIGURATION\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Batch size: {GPUConfig.batch_size} x {GPUConfig.gradient_accumulation} = {GPUConfig.batch_size * GPUConfig.gradient_accumulation}\")\n",
    "print(f\"Model: input={GPUConfig.input_dim}, hidden={GPUConfig.hidden_dim}\")\n",
    "print(f\"Dual decoder: {GPUConfig.use_dual_decoder} (primary: {GPUConfig.primary_decoder})\")\n",
    "print(f\"Gradient checkpointing: {GPUConfig.use_gradient_checkpointing}\")\n",
    "print(f\"Mixed precision (FP16): {GPUConfig.use_amp}\")\n",
    "print(f\"Dataset subset: {GPUConfig.subset_ratio*100:.0f}%\")\n",
    "print(f\"Augmentation: {GPUConfig.augmentation_intensity}\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ae848f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# IMPORT MODEL AND TRAINING COMPONENTS\n",
    "# ============================================================\n",
    "# Uses the updated v2 model with NaN-safe fixes\n",
    "\n",
    "from model_hybrid_v2 import (\n",
    "    HybridCTCAttentionModelV2,\n",
    "    create_hybrid_model_v2,\n",
    ")\n",
    "\n",
    "# Test model creation\n",
    "print(\"Creating model...\")\n",
    "model = HybridCTCAttentionModelV2(\n",
    "    input_dim=GPUConfig.input_dim,\n",
    "    hidden_dim=GPUConfig.hidden_dim,\n",
    "    vocab_size=73,  # Will be updated from vocabulary\n",
    "    encoder_layers=GPUConfig.encoder_layers,\n",
    "    decoder_layers=GPUConfig.decoder_layers,\n",
    "    num_heads=GPUConfig.num_heads,\n",
    "    dropout=GPUConfig.dropout,\n",
    "    use_dual_decoder=GPUConfig.use_dual_decoder,\n",
    "    primary_decoder=GPUConfig.primary_decoder,\n",
    "    use_gradient_checkpointing=GPUConfig.use_gradient_checkpointing\n",
    ").cuda()\n",
    "\n",
    "print()\n",
    "print(\"=\" * 60)\n",
    "print(\"MODEL SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Total parameters: {model.get_num_params():,}\")\n",
    "print(f\"Model size (FP32): {model.get_model_size_mb():.2f} MB\")\n",
    "print(f\"Estimated size (INT8): {model.get_model_size_mb()/4:.2f} MB\")\n",
    "print(f\"Dual decoder: {GPUConfig.use_dual_decoder}\")\n",
    "print(f\"Gradient checkpointing: {GPUConfig.use_gradient_checkpointing}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Quick forward pass test\n",
    "print(\"\\nTesting forward pass...\")\n",
    "with torch.no_grad():\n",
    "    x = torch.randn(4, 100, GPUConfig.input_dim).cuda()\n",
    "    lens = torch.tensor([100, 90, 80, 70]).cuda()\n",
    "    tgt = torch.randint(0, 73, (4, 20)).cuda()\n",
    "    outputs = model(x, lens, tgt, tf_ratio=0.5)\n",
    "    print(f\"CTC log probs: {outputs['ctc_log_probs'].shape}\")\n",
    "    print(f\"GRU outputs: {outputs['gru_outputs'].shape}\")\n",
    "    if 'tf_outputs' in outputs and outputs['tf_outputs'] is not None:\n",
    "        print(f\"Transformer outputs: {outputs['tf_outputs'].shape}\")\n",
    "        \n",
    "print(\"\\n‚úÖ Model test passed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7870b050",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# IMPORT TRAINING COMPONENTS\n",
    "# ============================================================\n",
    "from train_hybrid import (\n",
    "    SignLanguageDataset,\n",
    "    collate_fn,\n",
    "    HybridLoss,\n",
    "    LabelSmoothingLoss,\n",
    "    Trainer,\n",
    "    TrainConfig\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Training components imported successfully!\")\n",
    "print()\n",
    "print(\"Components loaded:\")\n",
    "print(\"  - SignLanguageDataset (with NaN filtering)\")\n",
    "print(\"  - HybridLoss (CTC + Attention with length validation)\")\n",
    "print(\"  - LabelSmoothingLoss (with clamping)\")\n",
    "print(\"  - Trainer (with gradient NaN detection)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9bea998",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CREATE TRAINER WITH CUSTOM CONFIG\n",
    "# ============================================================\n",
    "# Override TrainConfig with our GPU-specific paths\n",
    "\n",
    "# Monkey-patch the paths in TrainConfig\n",
    "TrainConfig.data_dir = GPUConfig.data_dir\n",
    "TrainConfig.csv_path = GPUConfig.csv_path\n",
    "TrainConfig.vocab_path = GPUConfig.vocab_path\n",
    "TrainConfig.checkpoint_dir = GPUConfig.checkpoint_dir\n",
    "TrainConfig.log_dir = GPUConfig.log_dir\n",
    "\n",
    "# Apply GPU-specific settings\n",
    "TrainConfig.batch_size = GPUConfig.batch_size\n",
    "TrainConfig.gradient_accumulation = GPUConfig.gradient_accumulation\n",
    "TrainConfig.num_workers = GPUConfig.num_workers\n",
    "\n",
    "# Create trainer\n",
    "print(\"Initializing trainer...\")\n",
    "trainer = Trainer(TrainConfig())\n",
    "\n",
    "print()\n",
    "print(\"=\" * 60)\n",
    "print(\"DATASET SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Train samples: {len(trainer.train_dataset):,}\")\n",
    "print(f\"Validation samples: {len(trainer.val_dataset):,}\")\n",
    "print(f\"Batches per epoch: {len(trainer.train_loader):,}\")\n",
    "print(f\"Vocabulary size: {len(trainer.vocab)}\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35d1fa95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# [OPTIONAL] RESUME FROM CHECKPOINT\n",
    "# ============================================================\n",
    "# Set RESUME_PATH to a checkpoint file to continue training\n",
    "\n",
    "RESUME_PATH = None  # e.g., \"./checkpoints_v2/latest.pth\"\n",
    "\n",
    "if RESUME_PATH and os.path.exists(RESUME_PATH):\n",
    "    print(f\"Resuming from checkpoint: {RESUME_PATH}\")\n",
    "    checkpoint = torch.load(RESUME_PATH, map_location=trainer.device)\n",
    "    trainer.model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    trainer.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    start_epoch = checkpoint.get('epoch', 0) + 1\n",
    "    print(f\"Resuming from epoch {start_epoch}\")\n",
    "else:\n",
    "    start_epoch = 1\n",
    "    print(\"Starting fresh training (no checkpoint loaded)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1329b7f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# START TRAINING\n",
    "# ============================================================\n",
    "# This will run for the configured number of epochs\n",
    "# Checkpoints are saved automatically\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"üöÄ STARTING TRAINING\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Epochs: {TrainConfig.epochs}\")\n",
    "print(f\"Batch size: {TrainConfig.batch_size} x {TrainConfig.gradient_accumulation}\")\n",
    "print(f\"Learning rate: {TrainConfig.learning_rate}\")\n",
    "print(f\"Device: {TrainConfig.device}\")\n",
    "print(f\"Mixed precision: {TrainConfig.use_amp}\")\n",
    "print(\"=\" * 60)\n",
    "print()\n",
    "print(\"Training will save checkpoints to:\", TrainConfig.checkpoint_dir)\n",
    "print(\"Press Ctrl+C to stop training (progress will be saved)\")\n",
    "print()\n",
    "\n",
    "# Run training\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d402ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# TRAINING COMPLETED - VIEW RESULTS\n",
    "# ============================================================\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"TRAINING COMPLETE\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "if hasattr(trainer, 'history') and trainer.history:\n",
    "    history = trainer.history\n",
    "    print(f\"Total epochs trained: {len(history.get('train_loss', []))}\")\n",
    "    print(f\"Best validation loss: {min(history.get('val_loss', [float('inf')])):.4f}\")\n",
    "    print(f\"Best validation accuracy: {max(history.get('val_acc', [0]))*100:.2f}%\")\n",
    "else:\n",
    "    print(\"No training history available\")\n",
    "\n",
    "print()\n",
    "print(\"Checkpoints saved to:\", TrainConfig.checkpoint_dir)\n",
    "print(\"  - best.pth (best validation loss)\")\n",
    "print(\"  - latest.pth (most recent)\")\n",
    "print(\"  - epoch_*.pth (periodic saves)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43e347b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# VISUALIZE TRAINING CURVES\n",
    "# ============================================================\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "if hasattr(trainer, 'history') and trainer.history:\n",
    "    history = trainer.history\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "    \n",
    "    # Total Loss\n",
    "    if 'train_loss' in history:\n",
    "        axes[0,0].plot(history['train_loss'], label='Train', linewidth=2)\n",
    "        axes[0,0].plot(history['val_loss'], label='Validation', linewidth=2)\n",
    "        axes[0,0].set_title('Total Loss', fontsize=12, fontweight='bold')\n",
    "        axes[0,0].set_xlabel('Epoch')\n",
    "        axes[0,0].set_ylabel('Loss')\n",
    "        axes[0,0].legend()\n",
    "        axes[0,0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Accuracy\n",
    "    if 'train_acc' in history:\n",
    "        axes[0,1].plot([a*100 for a in history['train_acc']], label='Train', linewidth=2)\n",
    "        axes[0,1].plot([a*100 for a in history['val_acc']], label='Validation', linewidth=2)\n",
    "        axes[0,1].set_title('Token Accuracy (%)', fontsize=12, fontweight='bold')\n",
    "        axes[0,1].set_xlabel('Epoch')\n",
    "        axes[0,1].set_ylabel('Accuracy (%)')\n",
    "        axes[0,1].legend()\n",
    "        axes[0,1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # CTC Loss\n",
    "    if 'train_ctc_loss' in history:\n",
    "        axes[1,0].plot(history['train_ctc_loss'], label='Train CTC', linewidth=2)\n",
    "        axes[1,0].plot(history['val_ctc_loss'], label='Val CTC', linewidth=2)\n",
    "        axes[1,0].set_title('CTC Loss', fontsize=12, fontweight='bold')\n",
    "        axes[1,0].set_xlabel('Epoch')\n",
    "        axes[1,0].set_ylabel('Loss')\n",
    "        axes[1,0].legend()\n",
    "        axes[1,0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # GRU Attention Loss\n",
    "    if 'train_gru_loss' in history:\n",
    "        axes[1,1].plot(history['train_gru_loss'], label='Train GRU', linewidth=2)\n",
    "        axes[1,1].plot(history['val_gru_loss'], label='Val GRU', linewidth=2)\n",
    "        axes[1,1].set_title('GRU Decoder Loss', fontsize=12, fontweight='bold')\n",
    "        axes[1,1].set_xlabel('Epoch')\n",
    "        axes[1,1].set_ylabel('Loss')\n",
    "        axes[1,1].legend()\n",
    "        axes[1,1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save figure\n",
    "    save_path = os.path.join(str(TrainConfig.checkpoint_dir), 'training_curves.png')\n",
    "    plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
    "    print(f\"Training curves saved to: {save_path}\")\n",
    "    \n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No training history available. Run training first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dabd510",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# QUICK INFERENCE TEST\n",
    "# ============================================================\n",
    "# Test the trained model on a sample from validation set\n",
    "\n",
    "# Load best checkpoint\n",
    "best_checkpoint = os.path.join(str(TrainConfig.checkpoint_dir), 'best.pth')\n",
    "\n",
    "if os.path.exists(best_checkpoint):\n",
    "    print(\"Loading best model for inference test...\")\n",
    "    \n",
    "    # Load checkpoint\n",
    "    checkpoint = torch.load(best_checkpoint, map_location='cuda')\n",
    "    trainer.model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    trainer.model.eval()\n",
    "    \n",
    "    # Get a random validation sample\n",
    "    import random\n",
    "    test_idx = random.randint(0, len(trainer.val_dataset) - 1)\n",
    "    sample = trainer.val_dataset[test_idx]\n",
    "    \n",
    "    # Prepare input\n",
    "    features = sample['src'].unsqueeze(0).cuda()\n",
    "    src_lens = torch.tensor([features.shape[1]]).cuda()\n",
    "    \n",
    "    # Run inference (greedy decoding)\n",
    "    with torch.no_grad():\n",
    "        # Get model outputs without teacher forcing\n",
    "        outputs = trainer.model(features, src_lens, tgt=None, tf_ratio=0.0)\n",
    "        \n",
    "        # Get predictions from primary decoder (GRU)\n",
    "        gru_logits = outputs['gru_outputs']  # (1, max_len, vocab_size)\n",
    "        predictions = gru_logits.argmax(dim=-1)  # (1, max_len)\n",
    "        \n",
    "        # Convert to text\n",
    "        pred_tokens = predictions[0].cpu().tolist()\n",
    "        \n",
    "        # Decode using vocabulary (reverse mapping)\n",
    "        idx_to_char = {v: k for k, v in trainer.vocab.items()}\n",
    "        pred_text = ''.join([idx_to_char.get(t, '?') for t in pred_tokens])\n",
    "        \n",
    "        # Remove padding and special tokens\n",
    "        if '<eos>' in pred_text:\n",
    "            pred_text = pred_text.split('<eos>')[0]\n",
    "        pred_text = pred_text.replace('<pad>', '').replace('<sos>', '')\n",
    "    \n",
    "    # Get ground truth\n",
    "    gt_tokens = sample['tgt'].tolist()\n",
    "    gt_text = ''.join([idx_to_char.get(t, '?') for t in gt_tokens])\n",
    "    gt_text = gt_text.replace('<pad>', '').replace('<sos>', '').replace('<eos>', '')\n",
    "    \n",
    "    print()\n",
    "    print(\"=\" * 60)\n",
    "    print(\"INFERENCE TEST\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"Sample index: {test_idx}\")\n",
    "    print(f\"Input shape: {features.shape}\")\n",
    "    print(f\"Ground truth: '{gt_text}'\")\n",
    "    print(f\"Prediction:   '{pred_text}'\")\n",
    "    print(\"=\" * 60)\n",
    "else:\n",
    "    print(f\"No checkpoint found at: {best_checkpoint}\")\n",
    "    print(\"Train the model first!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kortex (3.10.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
