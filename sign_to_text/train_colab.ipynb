{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "086c5cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU\n",
    "!nvidia-smi\n",
    "\n",
    "import torch\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfdce995",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cc683a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone your repo (or upload files)\n",
    "# Option 1: Clone from GitHub\n",
    "!git clone https://github.com/Surya-Narayan-M/kortex_5th_sem.git\n",
    "%cd kortex_5th_sem/sign_to_text\n",
    "\n",
    "# Option 2: If files are in Drive, copy them\n",
    "# !cp -r /content/drive/MyDrive/kortex_5th_sem ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "108db1b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install tqdm pandas numpy scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81728fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup paths for Colab\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add project to path\n",
    "sys.path.insert(0, '/content/kortex_5th_sem/sign_to_text')\n",
    "\n",
    "# Create symlinks or copy data\n",
    "# You need to upload your preprocessed data to Drive first!\n",
    "DRIVE_DATA_PATH = \"/content/drive/MyDrive/kortex_data\"  # Change this to your path\n",
    "\n",
    "# Check if data exists\n",
    "if os.path.exists(DRIVE_DATA_PATH):\n",
    "    print(f\"Data found at {DRIVE_DATA_PATH}\")\n",
    "    !ln -sf {DRIVE_DATA_PATH}/output_preprocessed /content/output_preprocessed\n",
    "    !ln -sf {DRIVE_DATA_PATH}/iSign_v1.1.csv /content/iSign_v1.1.csv\n",
    "else:\n",
    "    print(f\"⚠️ Data not found! Upload to: {DRIVE_DATA_PATH}\")\n",
    "    print(\"Required files:\")\n",
    "    print(\"  - output_preprocessed/ (folder with .npy files)\")\n",
    "    print(\"  - iSign_v1.1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b39aad23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modified training config for Colab\n",
    "import json\n",
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\n",
    "from torch.amp import autocast\n",
    "from tqdm.notebook import tqdm  # Notebook version of tqdm\n",
    "import logging\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s | %(message)s', datefmt='%H:%M:%S')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class ColabConfig:\n",
    "    \"\"\"Training configuration for Colab GPU (T4/V100)\"\"\"\n",
    "    \n",
    "    # Data paths - COLAB PATHS\n",
    "    data_dir = Path(\"/content/output_preprocessed\")\n",
    "    csv_path = Path(\"/content/iSign_v1.1.csv\")\n",
    "    vocab_path = Path(\"/content/kortex_5th_sem/sign_to_text/vocabulary.pkl\")\n",
    "    checkpoint_dir = Path(\"/content/drive/MyDrive/kortex_checkpoints\")  # Save to Drive!\n",
    "    \n",
    "    # Model architecture (same as local)\n",
    "    input_dim = 414\n",
    "    hidden_dim = 384\n",
    "    embedding_dim = 256\n",
    "    encoder_layers = 3\n",
    "    decoder_layers = 2\n",
    "    num_heads = 4\n",
    "    dropout = 0.3\n",
    "    \n",
    "    # Training - Colab optimized (T4 has 16GB, V100 has 16-32GB)\n",
    "    batch_size = 16  # Larger batch on Colab!\n",
    "    gradient_accumulation = 2  # Effective batch = 32\n",
    "    epochs = 100\n",
    "    learning_rate = 1e-3\n",
    "    min_lr = 1e-6\n",
    "    weight_decay = 1e-5\n",
    "    warmup_epochs = 5\n",
    "    \n",
    "    # CTC/Attention balance\n",
    "    ctc_weight_start = 0.5\n",
    "    ctc_weight_end = 0.2\n",
    "    ctc_weight_decay_epochs = 30\n",
    "    \n",
    "    # Label smoothing\n",
    "    label_smoothing = 0.1\n",
    "    \n",
    "    # Teacher forcing schedule\n",
    "    tf_start = 1.0\n",
    "    tf_end = 0.3\n",
    "    tf_decay_epochs = 40\n",
    "    \n",
    "    # Early stopping\n",
    "    patience = 15\n",
    "    min_delta = 0.001\n",
    "    \n",
    "    # Device\n",
    "    device = \"cuda\"\n",
    "    use_amp = True\n",
    "    \n",
    "    # Data loading - Colab can use more workers\n",
    "    num_workers = 2  # Colab has limited CPU\n",
    "    pin_memory = True\n",
    "    prefetch_factor = 2\n",
    "    persistent_workers = True\n",
    "    \n",
    "    # Sequence length limits\n",
    "    max_src_len = 500\n",
    "    max_tgt_len = 100\n",
    "    \n",
    "    # Validation split\n",
    "    val_ratio = 0.1\n",
    "    seed = 42\n",
    "\n",
    "print(\"Colab config loaded!\")\n",
    "print(f\"Batch size: {ColabConfig.batch_size} x {ColabConfig.gradient_accumulation} = {ColabConfig.batch_size * ColabConfig.gradient_accumulation}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9bea998",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import model and training components from your repo\n",
    "from model_hybrid import HybridCTCAttentionModel\n",
    "\n",
    "# Test model creation\n",
    "model = HybridCTCAttentionModel(\n",
    "    input_dim=414,\n",
    "    hidden_dim=384,\n",
    "    vocab_size=73\n",
    ").cuda()\n",
    "\n",
    "print(f\"Model parameters: {model.get_num_params():,}\")\n",
    "print(f\"Model size: {model.get_model_size_mb():.2f} MB\")\n",
    "\n",
    "# Quick forward test\n",
    "with torch.no_grad():\n",
    "    x = torch.randn(4, 100, 414).cuda()\n",
    "    lens = torch.tensor([100, 90, 80, 70]).cuda()\n",
    "    tgt = torch.randint(0, 73, (4, 20)).cuda()\n",
    "    ctc_out, attn_out = model(x, lens, tgt)\n",
    "    print(f\"CTC output: {ctc_out.shape}\")\n",
    "    print(f\"Attn output: {attn_out.shape}\")\n",
    "print(\"✅ Model works!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35d1fa95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import training components\n",
    "from train_hybrid import (\n",
    "    SignLanguageDataset, \n",
    "    collate_fn, \n",
    "    HybridLoss,\n",
    "    Trainer\n",
    ")\n",
    "\n",
    "# Patch the Trainer to use Colab config\n",
    "class ColabTrainer(Trainer):\n",
    "    def __init__(self):\n",
    "        # Use Colab config instead\n",
    "        super().__init__(ColabConfig())\n",
    "\n",
    "print(\"Training components imported!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1329b7f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create trainer and start training!\n",
    "trainer = ColabTrainer()\n",
    "\n",
    "print(f\"Train samples: {len(trainer.train_dataset)}\")\n",
    "print(f\"Val samples: {len(trainer.val_dataset)}\")\n",
    "print(f\"Batches per epoch: {len(trainer.train_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d402ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start training!\n",
    "# To resume from checkpoint: trainer.train(resume_from=\"/content/drive/MyDrive/kortex_checkpoints/latest.pth\")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43e347b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize training (run after training or during breaks)\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "history = trainer.history\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "\n",
    "# Loss\n",
    "axes[0,0].plot(history['train_loss'], label='Train')\n",
    "axes[0,0].plot(history['val_loss'], label='Val')\n",
    "axes[0,0].set_title('Total Loss')\n",
    "axes[0,0].legend()\n",
    "\n",
    "# Accuracy\n",
    "axes[0,1].plot([a*100 for a in history['train_acc']], label='Train')\n",
    "axes[0,1].plot([a*100 for a in history['val_acc']], label='Val')\n",
    "axes[0,1].set_title('Accuracy (%)')\n",
    "axes[0,1].legend()\n",
    "\n",
    "# CTC Loss\n",
    "axes[1,0].plot(history['train_ctc_loss'], label='Train CTC')\n",
    "axes[1,0].plot(history['val_ctc_loss'], label='Val CTC')\n",
    "axes[1,0].set_title('CTC Loss')\n",
    "axes[1,0].legend()\n",
    "\n",
    "# Attention Loss\n",
    "axes[1,1].plot(history['train_attn_loss'], label='Train Attn')\n",
    "axes[1,1].plot(history['val_attn_loss'], label='Val Attn')\n",
    "axes[1,1].set_title('Attention Loss')\n",
    "axes[1,1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('/content/drive/MyDrive/kortex_checkpoints/training_curves.png')\n",
    "plt.show()\n",
    "\n",
    "print(f\"Best Val Loss: {min(history['val_loss']):.4f}\")\n",
    "print(f\"Best Val Acc: {max(history['val_acc'])*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dabd510",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save final model to Drive\n",
    "!cp /content/kortex_5th_sem/sign_to_text/checkpoints_hybrid/* /content/drive/MyDrive/kortex_checkpoints/\n",
    "print(\"Checkpoints saved to Google Drive!\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
