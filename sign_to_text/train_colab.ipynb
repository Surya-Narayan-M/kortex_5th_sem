{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "086c5cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU\n",
    "!nvidia-smi\n",
    "\n",
    "import torch\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfdce995",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cc683a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone your repo (or upload files)\n",
    "# Option 1: Clone from GitHub\n",
    "!git clone https://github.com/Surya-Narayan-M/kortex_5th_sem.git\n",
    "%cd kortex_5th_sem/sign_to_text\n",
    "\n",
    "# Option 2: If files are in Drive, copy them\n",
    "# !cp -r /content/drive/MyDrive/kortex_5th_sem ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "108db1b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install tqdm pandas numpy scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81728fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup paths and preprocess data in Colab\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from scipy.ndimage import gaussian_filter1d\n",
    "\n",
    "# Add project to path\n",
    "sys.path.insert(0, '/content/kortex_5th_sem/sign_to_text')\n",
    "\n",
    "# ========== PATHS ==========\n",
    "DRIVE_RAW_DATA = \"/content/drive/MyDrive/kortex_data/output\"  # Raw landmarks (2.6 GB)\n",
    "OUTPUT_PREPROCESSED = \"/content/output_preprocessed\"  # Where preprocessed data goes\n",
    "CSV_PATH = \"/content/drive/MyDrive/kortex_data/iSign_v1.1.csv\"\n",
    "\n",
    "# Create output directory\n",
    "os.makedirs(OUTPUT_PREPROCESSED, exist_ok=True)\n",
    "\n",
    "# Check if raw data exists\n",
    "if not os.path.exists(DRIVE_RAW_DATA):\n",
    "    print(f\"âš ï¸ Raw data not found at {DRIVE_RAW_DATA}\")\n",
    "    print(\"Upload to: MyDrive/kortex_data/output/\")\n",
    "    print(\"(the 2.6 GB folder with .npy files from E:\\\\5thsem el\\\\output\\\\)\")\n",
    "else:\n",
    "    print(f\"âœ… Found raw data at {DRIVE_RAW_DATA}\")\n",
    "    print(f\"Files: {len(os.listdir(DRIVE_RAW_DATA))}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b39aad23",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ColabConfig:\n",
    "    \"\"\"Training configuration for Colab GPU (T4/V100)\"\"\"\n",
    "    \n",
    "    # Data paths - COLAB PATHS (HDF5 file instead of individual .npy files)\n",
    "    h5_file = Path(\"/content/dataset.h5\")  # â† NEW: HDF5 file path\n",
    "    csv_path = Path(\"/content/iSign_v1.1.csv\")\n",
    "    vocab_path = Path(\"/content/kortex_5th_sem/sign_to_text/vocabulary.pkl\")\n",
    "    checkpoint_dir = Path(\"/content/drive/MyDrive/kortex_checkpoints\")  # Save to Drive!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ae848f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== PREPROCESSING FUNCTIONS (from preprocess.py) ==========\n",
    "\n",
    "class PreprocessConfig:\n",
    "    smoothing_sigma = 1.0\n",
    "    min_hand_span = 0.01\n",
    "    add_velocity = True\n",
    "    add_acceleration = True\n",
    "\n",
    "def normalize_landmarks(landmarks: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Normalize landmarks to be person and camera invariant\"\"\"\n",
    "    T = landmarks.shape[0]\n",
    "    \n",
    "    # Reshape: hands (126) + shoulders (6) + elbows (6)\n",
    "    hands = landmarks[:, :126].reshape(T, 2, 21, 3).copy()\n",
    "    shoulders = landmarks[:, 126:132].reshape(T, 2, 3).copy()\n",
    "    elbows = landmarks[:, 132:138].reshape(T, 2, 3).copy()\n",
    "    \n",
    "    # 1. Normalize each hand to wrist origin\n",
    "    for h in range(2):\n",
    "        wrist = hands[:, h, 0:1, :].copy()\n",
    "        hands[:, h] = hands[:, h] - wrist\n",
    "    \n",
    "    # 2. Scale by hand span (wrist to middle fingertip)\n",
    "    for h in range(2):\n",
    "        span = np.linalg.norm(hands[:, h, 12] - hands[:, h, 0], axis=1, keepdims=True)\n",
    "        span = np.maximum(span, PreprocessConfig.min_hand_span)\n",
    "        hands[:, h] = hands[:, h] / span[:, :, np.newaxis]\n",
    "    \n",
    "    # 3. Normalize shoulders/elbows relative to body center\n",
    "    body_center = shoulders.mean(axis=1, keepdims=True)\n",
    "    shoulders = shoulders - body_center\n",
    "    elbows = elbows - body_center\n",
    "    \n",
    "    # 4. Scale body landmarks by shoulder width\n",
    "    shoulder_width = np.linalg.norm(shoulders[:, 0] - shoulders[:, 1], axis=1, keepdims=True)\n",
    "    shoulder_width = np.maximum(shoulder_width, PreprocessConfig.min_hand_span)\n",
    "    shoulders = shoulders / shoulder_width[:, :, np.newaxis]\n",
    "    elbows = elbows / shoulder_width[:, :, np.newaxis]\n",
    "    \n",
    "    # 5. Flatten back\n",
    "    normalized = np.concatenate([hands.reshape(T, -1), shoulders.reshape(T, -1), elbows.reshape(T, -1)], axis=1)\n",
    "    return normalized\n",
    "\n",
    "def compute_velocity(features: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Compute velocity (first-order derivative)\"\"\"\n",
    "    velocity = np.zeros_like(features)\n",
    "    velocity[1:] = features[1:] - features[:-1]\n",
    "    return velocity\n",
    "\n",
    "def compute_acceleration(velocity: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Compute acceleration (second-order derivative)\"\"\"\n",
    "    acceleration = np.zeros_like(velocity)\n",
    "    acceleration[1:] = velocity[1:] - velocity[:-1]\n",
    "    return acceleration\n",
    "\n",
    "def smooth_features(features: np.ndarray, sigma: float = 1.0) -> np.ndarray:\n",
    "    \"\"\"Apply Gaussian smoothing to reduce MediaPipe jitter\"\"\"\n",
    "    if sigma <= 0 or features.shape[0] < 3:\n",
    "        return features\n",
    "    return gaussian_filter1d(features, sigma=sigma, axis=0, mode='nearest')\n",
    "\n",
    "def preprocess_single_file(landmarks: np.ndarray, \n",
    "                           add_velocity: bool = True,\n",
    "                           add_acceleration: bool = True,\n",
    "                           smooth: bool = True) -> np.ndarray:\n",
    "    \"\"\"Complete preprocessing pipeline\"\"\"\n",
    "    if landmarks.shape[0] == 0:\n",
    "        return landmarks\n",
    "    \n",
    "    if landmarks.shape[0] < 3:\n",
    "        pad_length = 3 - landmarks.shape[0]\n",
    "        landmarks = np.pad(landmarks, ((0, pad_length), (0, 0)), mode='edge')\n",
    "    \n",
    "    # 1. Normalize\n",
    "    normalized = normalize_landmarks(landmarks)\n",
    "    \n",
    "    # 2. Smooth\n",
    "    if smooth:\n",
    "        normalized = smooth_features(normalized, sigma=PreprocessConfig.smoothing_sigma)\n",
    "    \n",
    "    # 3. Velocity\n",
    "    if add_velocity:\n",
    "        velocity = compute_velocity(normalized)\n",
    "        if smooth:\n",
    "            velocity = smooth_features(velocity, sigma=PreprocessConfig.smoothing_sigma)\n",
    "    \n",
    "    # 4. Acceleration\n",
    "    if add_acceleration and add_velocity:\n",
    "        acceleration = compute_acceleration(velocity)\n",
    "        if smooth:\n",
    "            acceleration = smooth_features(acceleration, sigma=PreprocessConfig.smoothing_sigma)\n",
    "    \n",
    "    # 5. Concatenate\n",
    "    features = [normalized]\n",
    "    if add_velocity:\n",
    "        features.append(velocity)\n",
    "    if add_acceleration and add_velocity:\n",
    "        features.append(acceleration)\n",
    "    \n",
    "    return np.concatenate(features, axis=1)\n",
    "\n",
    "print(\"âœ… Preprocessing functions loaded!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7870b050",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== RUN PREPROCESSING ==========\n",
    "import shutil\n",
    "\n",
    "# Check if preprocessing already done\n",
    "existing_files = len(os.listdir(OUTPUT_PREPROCESSED))\n",
    "raw_files = len(os.listdir(DRIVE_RAW_DATA))\n",
    "\n",
    "if existing_files == raw_files:\n",
    "    print(f\"âœ… Preprocessing already complete! {existing_files} files found.\")\n",
    "else:\n",
    "    print(f\"ðŸ”„ Starting preprocessing: {raw_files} files\")\n",
    "    print(f\"(This will take ~10-20 minutes depending on Colab speed)\")\n",
    "    \n",
    "    # Preprocess all files\n",
    "    for filename in tqdm(sorted(os.listdir(DRIVE_RAW_DATA))):\n",
    "        if not filename.endswith('.npy'):\n",
    "            continue\n",
    "        \n",
    "        input_path = os.path.join(DRIVE_RAW_DATA, filename)\n",
    "        output_path = os.path.join(OUTPUT_PREPROCESSED, filename)\n",
    "        \n",
    "        # Skip if already processed\n",
    "        if os.path.exists(output_path):\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            # Load raw landmarks\n",
    "            landmarks = np.load(input_path)  # (T, 138)\n",
    "            \n",
    "            # Preprocess\n",
    "            preprocessed = preprocess_single_file(landmarks)  # (T, 414)\n",
    "            \n",
    "            # Save\n",
    "            np.save(output_path, preprocessed)\n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸ Error processing {filename}: {e}\")\n",
    "    \n",
    "    print(f\"âœ… Preprocessing complete! {len(os.listdir(OUTPUT_PREPROCESSED))} files saved\")\n",
    "\n",
    "# Copy CSV to Colab if not there\n",
    "if not os.path.exists('/content/iSign_v1.1.csv'):\n",
    "    shutil.copy(CSV_PATH, '/content/iSign_v1.1.csv')\n",
    "    print(\"âœ… CSV copied to /content/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c755e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== CONVERT ALL PREPROCESSED DATA TO SINGLE HDF5 FILE ==========\n",
    "import h5py\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "H5_OUTPUT_PATH = \"/content/dataset.h5\"\n",
    "\n",
    "# Check if HDF5 already created\n",
    "if os.path.exists(H5_OUTPUT_PATH):\n",
    "    print(f\"âœ… HDF5 file already exists at {H5_OUTPUT_PATH}\")\n",
    "    with h5py.File(H5_OUTPUT_PATH, 'r') as f:\n",
    "        print(f\"   Samples in H5: {len(f['landmarks'])}\")\n",
    "        print(f\"   Feature dim: {f['landmarks'].shape[1]}\")\n",
    "else:\n",
    "    print(f\"ðŸ”„ Converting preprocessed .npy files to single HDF5 file...\")\n",
    "    print(f\"   Output: {H5_OUTPUT_PATH}\")\n",
    "    \n",
    "    # First, ensure all files are preprocessed\n",
    "    preprocessed_files = sorted([f for f in os.listdir(OUTPUT_PREPROCESSED) if f.endswith('.npy')])\n",
    "    print(f\"   Found {len(preprocessed_files)} preprocessed files\")\n",
    "    \n",
    "    # Create HDF5 file with chunked storage for efficiency\n",
    "    with h5py.File(H5_OUTPUT_PATH, 'w') as h5file:\n",
    "        # Pre-calculate total size for efficient allocation\n",
    "        total_frames = 0\n",
    "        file_info = []  # Store (filename, start_idx, num_frames)\n",
    "        \n",
    "        # First pass: scan all files to get sizes\n",
    "        print(\"   Scanning file sizes...\")\n",
    "        for filename in tqdm(preprocessed_files):\n",
    "            filepath = os.path.join(OUTPUT_PREPROCESSED, filename)\n",
    "            try:\n",
    "                data = np.load(filepath)\n",
    "                num_frames = data.shape[0]\n",
    "                file_info.append((filename, total_frames, num_frames))\n",
    "                total_frames += num_frames\n",
    "            except Exception as e:\n",
    "                print(f\"   âš ï¸ Error reading {filename}: {e}\")\n",
    "        \n",
    "        print(f\"   Total frames: {total_frames}\")\n",
    "        \n",
    "        # Create datasets with proper chunking\n",
    "        feature_dim = 414  # Your feature dimension\n",
    "        \n",
    "        # Main data storage\n",
    "        landmarks_dataset = h5file.create_dataset(\n",
    "            'landmarks',\n",
    "            shape=(total_frames, feature_dim),\n",
    "            dtype='float32',\n",
    "            chunks=(256, feature_dim),  # Chunk size: 256 frames at a time\n",
    "            compression='gzip',\n",
    "            compression_opts=4\n",
    "        )\n",
    "        \n",
    "        # Store file mapping for later retrieval\n",
    "        filenames_dataset = h5file.create_dataset(\n",
    "            'filenames',\n",
    "            (len(file_info),),\n",
    "            dtype=h5py.string_dtype(encoding='utf-8')\n",
    "        )\n",
    "        \n",
    "        # Store frame indices for each file (start, count)\n",
    "        frame_indices = h5file.create_dataset(\n",
    "            'frame_indices',\n",
    "            shape=(len(file_info), 2),\n",
    "            dtype='int32'\n",
    "        )\n",
    "        \n",
    "        # Second pass: load and store data\n",
    "        print(\"   Loading and storing data...\")\n",
    "        for idx, (filename, start_idx, num_frames) in enumerate(tqdm(file_info)):\n",
    "            filepath = os.path.join(OUTPUT_PREPROCESSED, filename)\n",
    "            data = np.load(filepath)\n",
    "            \n",
    "            # Store the data\n",
    "            landmarks_dataset[start_idx:start_idx + num_frames] = data\n",
    "            \n",
    "            # Store metadata\n",
    "            filenames_dataset[idx] = filename\n",
    "            frame_indices[idx] = [start_idx, num_frames]\n",
    "        \n",
    "        # Create metadata group\n",
    "        meta = h5file.create_group('metadata')\n",
    "        meta.attrs['total_samples'] = len(file_info)\n",
    "        meta.attrs['total_frames'] = total_frames\n",
    "        meta.attrs['feature_dim'] = feature_dim\n",
    "        meta.attrs['normalization'] = 'applied'\n",
    "        meta.attrs['has_velocity'] = True\n",
    "        meta.attrs['has_acceleration'] = True\n",
    "        \n",
    "        print(f\"âœ… HDF5 file created successfully!\")\n",
    "        print(f\"   File size: {os.path.getsize(H5_OUTPUT_PATH) / 1e9:.2f} GB\")\n",
    "        print(f\"   Total samples: {len(file_info)}\")\n",
    "        print(f\"   Total frames: {total_frames}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9bea998",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import model and training components from your repo\n",
    "from model_hybrid import HybridCTCAttentionModel\n",
    "\n",
    "# Test model creation\n",
    "model = HybridCTCAttentionModel(\n",
    "    input_dim=414,\n",
    "    hidden_dim=384,\n",
    "    vocab_size=73\n",
    ").cuda()\n",
    "\n",
    "print(f\"Model parameters: {model.get_num_params():,}\")\n",
    "print(f\"Model size: {model.get_model_size_mb():.2f} MB\")\n",
    "\n",
    "# Quick forward test\n",
    "with torch.no_grad():\n",
    "    x = torch.randn(4, 100, 414).cuda()\n",
    "    lens = torch.tensor([100, 90, 80, 70]).cuda()\n",
    "    tgt = torch.randint(0, 73, (4, 20)).cuda()\n",
    "    ctc_out, attn_out = model(x, lens, tgt)\n",
    "    print(f\"CTC output: {ctc_out.shape}\")\n",
    "    print(f\"Attn output: {attn_out.shape}\")\n",
    "print(\"âœ… Model works!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbbd8a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== HDF5 DATASET CLASS FOR EFFICIENT LOADING ==========\n",
    "import h5py\n",
    "from torch.utils.data import Dataset\n",
    "import pandas as pd\n",
    "\n",
    "class HDF5SignLanguageDataset(Dataset):\n",
    "    \"\"\"Dataset that loads landmarks from a single HDF5 file\"\"\"\n",
    "    \n",
    "    def __init__(self, h5_path, csv_path, vocab_path, max_src_len=500, max_tgt_len=100, \n",
    "                 indices=None, is_train=True):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            h5_path: Path to the HDF5 file\n",
    "            csv_path: Path to the CSV with sign labels\n",
    "            vocab_path: Path to vocabulary pickle file\n",
    "            max_src_len: Max source (landmark) sequence length\n",
    "            max_tgt_len: Max target (text) sequence length\n",
    "            indices: List of file indices to use (for train/val split)\n",
    "            is_train: Whether this is training set\n",
    "        \"\"\"\n",
    "        self.h5_path = h5_path\n",
    "        self.max_src_len = max_src_len\n",
    "        self.max_tgt_len = max_tgt_len\n",
    "        self.is_train = is_train\n",
    "        \n",
    "        # Load vocabulary\n",
    "        import pickle\n",
    "        with open(vocab_path, 'rb') as f:\n",
    "            self.vocab = pickle.load(f)\n",
    "        \n",
    "        # Load CSV\n",
    "        self.df = pd.read_csv(csv_path)\n",
    "        \n",
    "        # Get file mapping from HDF5\n",
    "        with h5py.File(h5_path, 'r') as f:\n",
    "            self.filenames = [f['filenames'][i].decode('utf-8') for i in range(len(f['filenames']))]\n",
    "            self.frame_indices = f['frame_indices'][:]  # (N_files, 2) with [start, count]\n",
    "        \n",
    "        # Use provided indices or all files\n",
    "        if indices is None:\n",
    "            self.indices = list(range(len(self.filenames)))\n",
    "        else:\n",
    "            self.indices = indices\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.indices)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        file_idx = self.indices[idx]\n",
    "        filename = self.filenames[file_idx]\n",
    "        \n",
    "        # Get frame range for this file\n",
    "        start_frame, num_frames = self.frame_indices[file_idx]\n",
    "        \n",
    "        # Load landmarks from HDF5\n",
    "        with h5py.File(self.h5_path, 'r') as f:\n",
    "            # Load all frames for this file\n",
    "            landmarks = f['landmarks'][start_frame:start_frame + num_frames]  # (T, 414)\n",
    "        \n",
    "        # Crop or pad to max_src_len\n",
    "        if landmarks.shape[0] > self.max_src_len:\n",
    "            landmarks = landmarks[:self.max_src_len]\n",
    "        else:\n",
    "            pad_len = self.max_src_len - landmarks.shape[0]\n",
    "            landmarks = np.pad(landmarks, ((0, pad_len), (0, 0)), mode='constant')\n",
    "        \n",
    "        # Get label from CSV (match filename to video_id)\n",
    "        video_id = filename.replace('.npy', '')\n",
    "        try:\n",
    "            row = self.df[self.df['video_id'] == video_id].iloc[0]\n",
    "            label = row['text']  # or 'gloss' depending on your CSV\n",
    "        except:\n",
    "            label = \"\"\n",
    "        \n",
    "        # Convert label to token indices\n",
    "        tokens = self.vocab.get(label, [])  # Returns list of token indices\n",
    "        tokens = tokens[:self.max_tgt_len]\n",
    "        \n",
    "        # Pad tokens\n",
    "        token_len = len(tokens)\n",
    "        tokens = np.array(tokens + [0] * (self.max_tgt_len - len(tokens)), dtype=np.int64)\n",
    "        \n",
    "        return {\n",
    "            'landmarks': torch.FloatTensor(landmarks),\n",
    "            'landmarks_len': torch.LongTensor([landmarks.shape[0]]),\n",
    "            'tokens': torch.LongTensor(tokens),\n",
    "            'tokens_len': torch.LongTensor([token_len]),\n",
    "            'video_id': video_id\n",
    "        }\n",
    "\n",
    "print(\"âœ… HDF5Dataset class loaded!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35d1fa95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import training components\n",
    "from train_hybrid import (\n",
    "    SignLanguageDataset, \n",
    "    collate_fn, \n",
    "    HybridLoss,\n",
    "    Trainer\n",
    ")\n",
    "\n",
    "# Patch the Trainer to use Colab config\n",
    "class ColabTrainer(Trainer):\n",
    "    def __init__(self):\n",
    "        # Use Colab config instead\n",
    "        super().__init__(ColabConfig())\n",
    "\n",
    "print(\"Training components imported!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1329b7f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create trainer and start training!\n",
    "trainer = ColabTrainer()\n",
    "\n",
    "print(f\"Train samples: {len(trainer.train_dataset)}\")\n",
    "print(f\"Val samples: {len(trainer.val_dataset)}\")\n",
    "print(f\"Batches per epoch: {len(trainer.train_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d402ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start training!\n",
    "# To resume from checkpoint: trainer.train(resume_from=\"/content/drive/MyDrive/kortex_checkpoints/latest.pth\")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43e347b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize training (run after training or during breaks)\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "history = trainer.history\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "\n",
    "# Loss\n",
    "axes[0,0].plot(history['train_loss'], label='Train')\n",
    "axes[0,0].plot(history['val_loss'], label='Val')\n",
    "axes[0,0].set_title('Total Loss')\n",
    "axes[0,0].legend()\n",
    "\n",
    "# Accuracy\n",
    "axes[0,1].plot([a*100 for a in history['train_acc']], label='Train')\n",
    "axes[0,1].plot([a*100 for a in history['val_acc']], label='Val')\n",
    "axes[0,1].set_title('Accuracy (%)')\n",
    "axes[0,1].legend()\n",
    "\n",
    "# CTC Loss\n",
    "axes[1,0].plot(history['train_ctc_loss'], label='Train CTC')\n",
    "axes[1,0].plot(history['val_ctc_loss'], label='Val CTC')\n",
    "axes[1,0].set_title('CTC Loss')\n",
    "axes[1,0].legend()\n",
    "\n",
    "# Attention Loss\n",
    "axes[1,1].plot(history['train_attn_loss'], label='Train Attn')\n",
    "axes[1,1].plot(history['val_attn_loss'], label='Val Attn')\n",
    "axes[1,1].set_title('Attention Loss')\n",
    "axes[1,1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('/content/drive/MyDrive/kortex_checkpoints/training_curves.png')\n",
    "plt.show()\n",
    "\n",
    "print(f\"Best Val Loss: {min(history['val_loss']):.4f}\")\n",
    "print(f\"Best Val Acc: {max(history['val_acc'])*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dabd510",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save final model to Drive\n",
    "!cp /content/kortex_5th_sem/sign_to_text/checkpoints_hybrid/* /content/drive/MyDrive/kortex_checkpoints/\n",
    "print(\"Checkpoints saved to Google Drive!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kortex (3.10.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
